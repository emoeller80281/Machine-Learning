{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1380f63",
   "metadata": {},
   "source": [
    "# Attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69825ac9",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "The dot product allows us to calculate the similarity between two vectors. In scaled dot-product attention, this is used to determine how similar a **Query** vector for one token is to the **Key** vector of another token.\n",
    "\n",
    "The dot product is calculated by multiplying each component of two vectors, then summing each of them.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{a} = [a_{1}, a_{2}, a_{3}, ..., a_{n}] \\\\\n",
    "\\vec{b} = [b_{1}, b_{2}, b_{3}, ..., b_{n}] \\\\\n",
    "\\vec{a} \\cdot \\vec{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + ... + a_{n}b_{n} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n}a_{i}b_{i}$$\n",
    "\n",
    "This can also be written as matrix multiplying between vector $a$ and the transform of vector $b$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430d8534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
      "b = [0.15599452 0.05808361 0.86617615 0.60111501 0.70807258]\n",
      "\n",
      "0.37 * 0.16 = 0.06\n",
      "0.95 * 0.06 = 0.06\n",
      "0.73 * 0.87 = 0.63\n",
      "0.60 * 0.60 = 0.36\n",
      "0.16 * 0.71 = 0.11\n",
      "___________________\n",
      "              1.22\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import visualization as vis\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "a = np.random.random(5)\n",
    "b = np.random.random(5)\n",
    "dot_product = [i*x for i, x in zip(a, b)]\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print()\n",
    "for i, product in enumerate(dot_product):\n",
    "    print(f\"{a[i]:.2f} * {b[i]:.2f} = {product:.2f}\")\n",
    "\n",
    "print(\"___________________\")\n",
    "print(f\"              {sum(dot_product):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d2ae8",
   "metadata": {},
   "source": [
    "This gives us the similarity between the two vectors. \n",
    "\n",
    "We can also calculate the dot product in Numpy using `np.dot(a, b)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c5c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.dot(a, b):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585104b",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065701ff",
   "metadata": {},
   "source": [
    "Let's look at how we can calculate the dot product between matrices built using multiple Key - Query vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3fcb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence split by word: ['machine', 'learning', 'is', 'fun']\n",
      "\n",
      "Word to index mapping: {'machine': 0, 'learning': 1, 'is': 2, 'fun': 3}\n",
      "\n",
      "Query Matrix:\n",
      "             Dim 1     Dim 2     Dim 3     Dim 4     Dim 5     Dim 6\n",
      "machine   0.020584  0.969910  0.832443  0.212339  0.181825  0.183405\n",
      "learning  0.304242  0.524756  0.431945  0.291229  0.611853  0.139494\n",
      "is        0.292145  0.366362  0.456070  0.785176  0.199674  0.514234\n",
      "fun       0.592415  0.046450  0.607545  0.170524  0.065052  0.948886\n",
      "\n",
      "Key Matrix:\n",
      "             Dim 1     Dim 2     Dim 3     Dim 4     Dim 5     Dim 6\n",
      "machine   0.965632  0.808397  0.304614  0.097672  0.684233  0.440152\n",
      "learning  0.122038  0.495177  0.034389  0.909320  0.258780  0.662522\n",
      "is        0.311711  0.520068  0.546710  0.184854  0.969585  0.775133\n",
      "fun       0.939499  0.894827  0.597900  0.921874  0.088493  0.195983\n"
     ]
    }
   ],
   "source": [
    "# Start with an example sentence. Each word will be a token and get a key and query vector\n",
    "example_sentence=\"machine learning is fun\".split()\n",
    "print(f\"Example sentence split by word: {example_sentence}\")\n",
    "\n",
    "# We map each word to an index\n",
    "word_to_idx = dict([(example_sentence[i], i) for i in range(len(example_sentence))])\n",
    "print(f\"\\nWord to index mapping: {word_to_idx}\")\n",
    "\n",
    "# Next, we can build key, query, and value vectors with 5 random numbers per word\n",
    "# Each ROW corresponds to a WORD and each COLUMN corresponds to an EMBEDDING\n",
    "EMBEDDING_DIM=6\n",
    "CONTEXT_LENGTH=len(example_sentence)\n",
    "query_vectors = np.array([np.random.random(EMBEDDING_DIM) for i in example_sentence])\n",
    "key_vectors = np.array([np.random.random(EMBEDDING_DIM) for i in example_sentence])\n",
    "value_vectors = np.array([np.random.random(EMBEDDING_DIM) for i in example_sentence])\n",
    "\n",
    "column_map = {i: f\"Dim {i+1}\" for i in range(EMBEDDING_DIM)}\n",
    "\n",
    "# Let's print out the words and their embeddings\n",
    "def convert_embedding_to_df(matrix, word_to_idx):\n",
    "    \"\"\"Helper function to convert a matrix where each row is a word embedding to a Pandas df\"\"\"\n",
    "    matrix_df = pd.DataFrame()\n",
    "\n",
    "    for i, word in enumerate(word_to_idx):\n",
    "        matrix_df[word] = matrix[i]\n",
    "\n",
    "    matrix_df = matrix_df.T.rename(columns=column_map)\n",
    "    \n",
    "    return matrix_df\n",
    "\n",
    "print(\"\\nQuery Matrix:\")\n",
    "query_vector_df = convert_embedding_to_df(query_vectors, word_to_idx)\n",
    "print(query_vector_df.head())\n",
    "\n",
    "print(\"\\nKey Matrix:\")\n",
    "key_vector_df = convert_embedding_to_df(key_vectors, word_to_idx)\n",
    "print(key_vector_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c58d1",
   "metadata": {},
   "source": [
    "Now that we have the query and key matrices, we want to get the **Dot Product** between the Query and Key vectors for each word. As above, we want to calculate $query_{1}key_{1} + query_{2}key_{2} + ... + query_{n}key_{n}$. To do this, we need to **Transpose** the key matrix. Otherwise, the matrix multiplication would multiply each row of the query matrix with each column of the value matrix, which is not what we want.\n",
    "\n",
    "$$\\vec{\\text{Query}} \\cdot \\vec{\\text{Key}} = \\text{Query} \\times \\text{Key}^{T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2017f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           machine  learning        is       fun\n",
      "machine   1.283399  0.873062  1.323650  1.632743\n",
      "learning  1.358068  0.827404  1.359098  1.363621\n",
      "is        1.156850  1.339090  1.268280  1.717269\n",
      "fun       1.273492  0.916744  1.371078  1.310313\n"
     ]
    }
   ],
   "source": [
    "matrix_dot_product = np.dot(query_vectors, key_vectors.T)\n",
    "\n",
    "# We can represent the result as the similarity between each words Query vector to each other words Key vector\n",
    "def convert_query_key_dot_product_to_df(matrix_dot_product, word_to_idx):\n",
    "    return pd.DataFrame(data=matrix_dot_product, index=[i for i in word_to_idx], columns =[i for i in word_to_idx])\n",
    "matrix_dot_product_df = convert_query_key_dot_product_to_df(matrix_dot_product, word_to_idx)\n",
    "print(matrix_dot_product_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648c458",
   "metadata": {},
   "source": [
    "The results of this matrix are then scaled by dividing the dot product by the square root of the number of embedding dimensions for the keys, or $d_{k}$. Each key consists of a vector of 5 numbers, so we would divide by the square root of 5. This scaling is done to counteract the vanishing gradient effects of calculating softmax values for large values of $d_{k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bba77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           machine  learning        is       fun\n",
      "machine   0.523946  0.356426  0.540378  0.666564\n",
      "learning  0.554429  0.337786  0.554849  0.556696\n",
      "is        0.472282  0.546681  0.517773  0.701072\n",
      "fun       0.519901  0.374259  0.559740  0.534933\n"
     ]
    }
   ],
   "source": [
    "scaled_matrix_dot_product = matrix_dot_product / np.sqrt(EMBEDDING_DIM)\n",
    "scaled_matrix_dot_product_df = convert_query_key_dot_product_to_df(scaled_matrix_dot_product, word_to_idx)\n",
    "print(scaled_matrix_dot_product_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff61cdb",
   "metadata": {},
   "source": [
    "A **Softmax** function is then applied to the scaled dot product matrix to isolate important values. The softmax function converts the values into a range of probabilities between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0497d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           machine  learning        is       fun\n",
      "machine   0.062476  0.052840  0.063511  0.072053\n",
      "learning  0.064410  0.051864  0.064437  0.064556\n",
      "is        0.059330  0.063913  0.062092  0.074583\n",
      "fun       0.062224  0.053791  0.064753  0.063166\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "attention_weights = softmax(scaled_matrix_dot_product)\n",
    "attention_weights_df = convert_query_key_dot_product_to_df(attention_weights, word_to_idx)\n",
    "print(attention_weights_df)\n",
    "\n",
    "# We can also represent the similarities between the queries and keys as a heatmap\n",
    "vis.show_heatmaps(attention_weights.reshape((1, 1, 4, 4)), \"Queries\", \"Keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821cf88",
   "metadata": {},
   "source": [
    "This matrix, called the **attention value** matrix, represents how similar each word is to one another based on their Query and Key vectors. These are learned from the data, which allows the algorithm to learn to associate similar words. \n",
    "\n",
    "The matrix of value vectors are then used as weights for the value vector. You can think of the Keys and Queries as asking, \"which of the words are most relevant to one another?\", while the value is relevant return value. For translation, the value might correspond to a word in a different language. If the Query for the word \"machine\" matches closely with the Key for \"learning\", and the value for \"learning\" in a trained German translation model would correspond with \"lernen\".\n",
    "\n",
    "In summary, Queries as the question: \"where is the information that's relevent to me?\" the Keys answer: \"I am ___ relevant to you\" and the Values contain the information that is passed forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c8d9987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      "             Dim 1     Dim 2     Dim 3     Dim 4     Dim 5     Dim 6\n",
      "machine   0.122289  0.066957  0.057909  0.119481  0.162798  0.165646\n",
      "learning  0.117035  0.066686  0.055841  0.119109  0.158512  0.161375\n",
      "is        0.126112  0.071848  0.059146  0.126645  0.162197  0.175994\n",
      "fun       0.116649  0.066980  0.054766  0.120158  0.155868  0.161861\n"
     ]
    }
   ],
   "source": [
    "attention_output = np.matmul(attention_weights, value_vectors)\n",
    "\n",
    "attention_output_df = convert_embedding_to_df(attention_output, word_to_idx)\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9193c9",
   "metadata": {},
   "source": [
    "## Multi-headed attention\n",
    "\n",
    "Before we move forward, let's define a more robust implementation of the attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30158545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(query_matrix, key_matrix, value_matrix):\n",
    "    matrix_dot_product = np.dot(query_matrix, key_matrix.T)                     # 1. Calculate Query / Key similarity matrix\n",
    "    scaled_matrix_dot_product = matrix_dot_product / np.sqrt(EMBEDDING_DIM)     # 2. Scale by embedding size to avoid gradients vanishing\n",
    "    attention_weights = softmax(scaled_matrix_dot_product)                      # 3. Softmax to get most important relationships btw each word\n",
    "    attention_output = np.matmul(attention_weights, value_matrix)               # 4. Get the values between each feature scaled by the importance\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c95ae",
   "metadata": {},
   "source": [
    "Let's make sure this function is still giving the same output as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1384f120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      "             Dim 1     Dim 2     Dim 3     Dim 4     Dim 5     Dim 6\n",
      "machine   0.122289  0.066957  0.057909  0.119481  0.162798  0.165646\n",
      "learning  0.117035  0.066686  0.055841  0.119109  0.158512  0.161375\n",
      "is        0.126112  0.071848  0.059146  0.126645  0.162197  0.175994\n",
      "fun       0.116649  0.066980  0.054766  0.120158  0.155868  0.161861\n"
     ]
    }
   ],
   "source": [
    "attention_func_output = attention_block(query_vectors, key_vectors, value_vectors)\n",
    "\n",
    "attention_func_output_df = convert_embedding_to_df(attention_func_output, word_to_idx)\n",
    "print(\"Attention Output:\")\n",
    "print(attention_func_output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8d494",
   "metadata": {},
   "source": [
    "The goal of multi-headed attention is to improve our model by giving itthe ability to focus on different aspects of the input that could be relevant to the overall context, then combining these insights together into a single, deeper understanding of the relationships between concepts. We accomplish this by passing the query, key, and value matrices through multiple attention functions called **attention heads** at once, then concatenating their outputs together. However, if we just pass in the same query, key, and value matrices into the attention heads then we will get the same output from each of them. To allow each head to focus on different aspects of the overall context, we pass the query, key, and value matrices through trainable **weight matrices**. These are called **independently learned linear projections**, as they are taking the values of the vector matrices and modifying them by different weights. The projects are passed into the attention heads, the output is concatenated, then a final linear projection produces the final output. The notation for this equation is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MuliHead(Q, K, V)} = \\text{Concat}(\\text{head}_{1},...,\\text{head}_{h})W^{O} \\\\\n",
    "\\text{where head}_{i} = \\text{Attention}(QW_{i}^{Q}, Kw_{i}^{K}, VW_{i}^{V}) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where the projections are parameter matrices $W^{Q}_{i} \\in \\mathbb{R}^{d_{model} \\times d_{k}} $, $W^{K}_{i} \\in \\mathbb{R}^{d_{model} \\times d_{k}}$, $W^{V}_{i} \\in \\mathbb{R}^{d_{model} \\times d_{v}}$ and $W^{O} \\in \\mathbb{R}^{hd_{v} \\times d_{model}}$.\n",
    "\n",
    "The original implementation of the transformer model used 8 attention heads. For this implementation, we will use 2 heads to keep things simple.\n",
    "\n",
    "our $d_{model}$ value is the same as the embedding dimension of 6.\n",
    "\n",
    "The information is split and passed to head head, with a size equal to the embedding size / number of heads.\n",
    "\n",
    "Therefore, $d_{k} = d_{v} = d_{model} / h$ Since $d_{model}$ = 6 and we are using 2 heads, the dimensionality of $d_{k}$ and $d_{v}$ will be 6 x 3.\n",
    "\n",
    "After contatenating the output from each of the heads, we multiply by a weight matrix of shape $(h * d_{v}) \\times d_{model}$. We have 3 heads and the dimensionality of $d_{v}$ is 3, $(h * d_{v})$ = 6. Therefore, $W^{O}$ will be a 6 x 6 matrix.\n",
    "\n",
    "The output from our multheaded attention is a 4 x 6 matrix, and $W^{O}$ is a 6 x 6 matrix, so the final output will be a 4 x 6 matrix.\n",
    "\n",
    "Note that this process retains the dimensionality of the original input embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ee35a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output weight matrix shape: (6, 6)\n",
      "\n",
      "Multihead Attention Output:\n",
      "[[1.07043584 1.62960896 1.45838598 1.44572424 1.15413241 1.08176137]\n",
      " [0.82305966 1.27220231 1.13934366 1.11597382 0.88935979 0.83424004]\n",
      " [0.8847558  1.45890057 1.30822418 1.21902181 0.96378    0.90930374]\n",
      " [0.52724468 0.67842851 0.60493516 0.68558162 0.55757996 0.51625061]]\n",
      "\n",
      "Multihead Attention output shape: (4, 6)\n"
     ]
    }
   ],
   "source": [
    "NUM_ATTENTION_HEADS=2\n",
    "import math\n",
    "HEAD_SIZE = EMBEDDING_DIM // NUM_ATTENTION_HEADS # Specify the size of the embeddings (cols) for each head\n",
    "\n",
    "def attention_head(query_matrix, key_matrix, value_matrix):\n",
    "    # First, we need to set up random weight matrices for the linear projections\n",
    "    query_weight_matrix = np.random.rand(EMBEDDING_DIM, HEAD_SIZE)       # embed x head size (6 x 3)\n",
    "    key_weight_matrix = np.random.rand(EMBEDDING_DIM, HEAD_SIZE)         # embed x head size (6 x 3)\n",
    "    value_weight_matrix = np.random.rand(EMBEDDING_DIM, HEAD_SIZE)       # embed x head size (6 x 3)\n",
    "\n",
    "    # The query, key, and values are multiplied by their weight matrices\n",
    "    query_linear_embed = np.matmul(query_vectors, query_weight_matrix)      # input x head size (4 x 3)\n",
    "    key_linear_embed = np.matmul(key_vectors, key_weight_matrix)            # input x head size (4 x 3)\n",
    "    value_linear_embed = np.matmul(value_vectors, value_weight_matrix)      # input x head size (4 x 3)\n",
    "    \n",
    "    # Then, these linear embeddings are passed through the attention block\n",
    "    attention_output = attention_block(query_linear_embed, key_linear_embed, value_linear_embed) # input x head size (4 x 3)\n",
    "    \n",
    "    return attention_output\n",
    "\n",
    "# Run attention heads and appended their outputs to a list\n",
    "attention_head_output_list=[]\n",
    "for i in range(NUM_ATTENTION_HEADS):\n",
    "    attention_head_output = attention_head(query_vectors, key_vectors, value_vectors)\n",
    "    attention_head_output_list.append(attention_head_output)\n",
    "\n",
    "# The outputs from each attention head are concatenated together\n",
    "concat_attention_head_output = np.concatenate(attention_head_output_list, axis=1) # input x (head size * num heads) (4 x 6)\n",
    "\n",
    "# Finally, the concatenated output is multiplied by another linear projection weight matrix\n",
    "num_rows = HEAD_SIZE * NUM_ATTENTION_HEADS  # (3 * 2) = 6\n",
    "num_cols = EMBEDDING_DIM                    # 6\n",
    "\n",
    "# \n",
    "output_weight_matrix = np.random.rand(num_rows, num_cols)   # (num heads * head size) x embed (6 x 6)\n",
    "print(f\"Output weight matrix shape: {output_weight_matrix.shape}\")\n",
    "\n",
    "multihead_attention_output = concat_attention_head_output @ output_weight_matrix\n",
    "\n",
    "print(f\"\\nMultihead Attention Output:\\n{multihead_attention_output}\")\n",
    "print(f\"\\nMultihead Attention output shape: {multihead_attention_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d69d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
